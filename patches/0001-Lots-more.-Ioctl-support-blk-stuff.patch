From b33fbb4c371be4ec9591f798285cb59594e4d55e Mon Sep 17 00:00:00 2001
From: wyjw <yujian.wu1@gmail.com>
Date: Tue, 27 Oct 2020 23:59:14 +0800
Subject: [PATCH] Lots more. Ioctl support, blk stuff.

---
 block/blk-exec.c                       |    3 +-
 block/blk-mq.c                         |    4 +
 drivers/nvme/host/Kconfig              |    6 +
 drivers/nvme/host/Makefile             |    1 +
 drivers/nvme/host/api/treenvme_ioctl.h |   81 ++
 drivers/nvme/host/core.c               |  146 +++-
 drivers/nvme/host/depthpath.c          | 1006 ++++++++++++++++++++++++
 drivers/nvme/host/nvme.h               |  186 +++++
 drivers/nvme/host/pci.c                |  183 ++---
 drivers/nvme/host/tokuspec.h           |  115 +++
 include/linux/blkdev.h                 |    4 +
 include/uapi/linux/treenvme.h          |    0
 include/uapi/linux/treenvme_ioctl.h    |    1 +
 13 files changed, 1614 insertions(+), 122 deletions(-)
 create mode 100644 drivers/nvme/host/api/treenvme_ioctl.h
 create mode 100644 drivers/nvme/host/depthpath.c
 create mode 100644 drivers/nvme/host/tokuspec.h
 create mode 100644 include/uapi/linux/treenvme.h
 create mode 120000 include/uapi/linux/treenvme_ioctl.h

diff --git a/block/blk-exec.c b/block/blk-exec.c
index e20a852ae432..0d9d6a2e74a5 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -27,7 +27,8 @@ static void blk_end_sync_rq(struct request *rq, blk_status_t error)
 	 * complete last, if this is a stack request the process (and thus
 	 * the rq pointer) could be invalid right after this complete()
 	 */
-	complete(waiting);
+	if (waiting != NULL)
+		complete(waiting);
 }
 
 /**
diff --git a/block/blk-mq.c b/block/blk-mq.c
index a7785df2c944..999fd29c61f8 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -2016,6 +2016,10 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	blk_mq_bio_to_request(rq, bio, nr_segs);
 
+	// alter
+	rq->alter_count = 0;
+	rq->total_count = 32;
+
 	plug = blk_mq_plug(q, bio);
 	if (unlikely(is_flush_fua)) {
 		/* Bypass scheduler for flush requests */
diff --git a/drivers/nvme/host/Kconfig b/drivers/nvme/host/Kconfig
index 9c17ed32be64..e2e6272c981b 100644
--- a/drivers/nvme/host/Kconfig
+++ b/drivers/nvme/host/Kconfig
@@ -24,6 +24,12 @@ config NVME_MULTIPATH
 	   /dev/nvmeXnY device will show up for each NVMe namespaces,
 	   even if it is accessible through multiple controllers.
 
+config TREENVME
+	bool "treeNvme support"
+	depends on PCI && BLOCK && NVME_CORE
+	---help---
+	   This option allows for treenvme.
+
 config NVME_HWMON
 	bool "NVMe hardware monitoring"
 	depends on (NVME_CORE=y && HWMON=y) || (NVME_CORE=m && HWMON)
diff --git a/drivers/nvme/host/Makefile b/drivers/nvme/host/Makefile
index fc7b26be692d..c26ffc6574ef 100644
--- a/drivers/nvme/host/Makefile
+++ b/drivers/nvme/host/Makefile
@@ -15,6 +15,7 @@ nvme-core-$(CONFIG_NVME_MULTIPATH)	+= multipath.o
 nvme-core-$(CONFIG_NVM)			+= lightnvm.o
 nvme-core-$(CONFIG_FAULT_INJECTION_DEBUG_FS)	+= fault_inject.o
 nvme-core-$(CONFIG_NVME_HWMON)		+= hwmon.o
+nvme-core-$(CONFIG_TREENVME)		+= depthpath.o
 
 nvme-y					+= pci.o
 
diff --git a/drivers/nvme/host/api/treenvme_ioctl.h b/drivers/nvme/host/api/treenvme_ioctl.h
new file mode 100644
index 000000000000..13627fef2fd2
--- /dev/null
+++ b/drivers/nvme/host/api/treenvme_ioctl.h
@@ -0,0 +1,81 @@
+#ifndef _UAPI_LINUX_TREENVME_IOCTL_H
+#define _UAPI_LINUX_TREENVME_IOCTL_H
+
+#include <linux/types.h>
+
+#define TREENVME_IOCTL '$'
+
+// taken from the original NVME ioctl
+struct treenvme_user_io {
+	__u8	opcode;
+	__u8	flags;
+	__u16	control;
+	__u16	nblocks;
+	__u16	rsvd;
+	__u64	metadata;
+	__u64	addr;
+	__u64	slba;
+	__u32	dsmgmt;
+	__u32	reftag;
+	__u16	apptag;
+	__u16	appmask;
+};
+
+struct treenvme_passthru_cmd {
+	__u8	opcode;
+	__u8	flags;
+	__u16	rsvd1;
+	__u32	nsid;
+	__u32	cdw2;
+	__u32	cdw3;
+	__u64	metadata;
+	__u64	addr;
+	__u32	metadata_len;
+	__u32	data_len;
+	__u32	cdw10;
+	__u32	cdw11;
+	__u32	cdw12;
+	__u32	cdw13;
+	__u32	cdw14;
+	__u32	cdw15;
+	__u32	timeout_ms;
+	__u32	result;
+};
+
+enum treenvme_translation_type {
+	TREENVME_TRANSLATION_NONE = 0,
+	TREENVME_TRANSLATION_CURRENT,
+	TREENVME_TRANSLATION_INPROGRESS,
+	TREENVME_TRANSLATION_CHECKPOINTED,
+	TREENVME_TRANSLATION_DEBUG
+};
+
+struct treenvme_block_translation_pair {
+	union {
+		uint64_t diskoff;
+		uint32_t free_blocknum;	
+	} u;
+
+	uint64_t size;
+};
+
+struct treenvme_block_table {
+	enum treenvme_translation_type type;
+	int64_t length_of_array;
+	uint32_t smallest;
+	uint32_t next_head;
+	struct treenvme_block_translation_pair *block_translation; 
+};
+
+struct treenvme_params {
+	uint32_t flags;
+	uint32_t num;
+};
+
+// possible IOCTL commands
+#define TREENVME_IOCTL_ID			_IO('$', 0x50)
+#define TREENVME_IOCTL_SUBMIT_IO 		_IOWR('$', 0x51, struct treenvme_user_io)
+#define TREENVME_IOCTL_IO_CMD 			_IOWR('$', 0x52, struct treenvme_passthru_cmd)
+#define TREENVME_IOCTL_SETUP 			_IOWR('$', 0x53, struct treenvme_params)
+#define TREENVME_IOCTL_REGISTER_BLOCKTABLE	_IOWR('$', 0x54, struct treenvme_block_table)
+#endif
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index f3c037f5a9ba..2eb3d8b91be6 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -776,6 +776,13 @@ blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 	}
 
 	cmd->common.command_id = req->tag;
+
+	// alter
+	if (req)
+	{
+		req->first_command_id = cmd->common.command_id;
+	}
+	
 	trace_nvme_setup_cmd(req, cmd);
 	return ret;
 }
@@ -888,7 +895,7 @@ static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
 	return ERR_PTR(ret);
 }
 
-static int nvme_submit_user_cmd(struct request_queue *q,
+int nvme_submit_user_cmd(struct request_queue *q,
 		struct nvme_command *cmd, void __user *ubuffer,
 		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
 		u32 meta_seed, u64 *result, unsigned timeout)
@@ -945,6 +952,7 @@ static int nvme_submit_user_cmd(struct request_queue *q,
 	blk_mq_free_request(req);
 	return ret;
 }
+EXPORT_SYMBOL_GPL(nvme_submit_user_cmd);
 
 static void nvme_keep_alive_end_io(struct request *rq, blk_status_t status)
 {
@@ -1143,7 +1151,7 @@ static int nvme_identify_ns_list(struct nvme_ctrl *dev, unsigned nsid, __le32 *n
 				    NVME_IDENTIFY_DATA_SIZE);
 }
 
-static int nvme_identify_ns(struct nvme_ctrl *ctrl,
+int nvme_identify_ns(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns **id)
 {
 	struct nvme_command c = { };
@@ -1166,6 +1174,7 @@ static int nvme_identify_ns(struct nvme_ctrl *ctrl,
 
 	return error;
 }
+EXPORT_SYMBOL_GPL(nvme_identify_ns);
 
 static int nvme_features(struct nvme_ctrl *dev, u8 op, unsigned int fid,
 		unsigned int dword11, void *buffer, size_t buflen, u32 *result)
@@ -1870,7 +1879,7 @@ static void nvme_update_disk_info(struct gendisk *disk,
 	blk_mq_unfreeze_queue(disk->queue);
 }
 
-static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
+void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
 {
 	struct nvme_ns *ns = disk->private_data;
 
@@ -1908,6 +1917,7 @@ static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
 	}
 #endif
 }
+EXPORT_SYMBOL_GPL(__nvme_revalidate_disk);
 
 static int nvme_revalidate_disk(struct gendisk *disk)
 {
@@ -2115,6 +2125,18 @@ const struct block_device_operations nvme_ns_head_ops = {
 };
 #endif /* CONFIG_NVME_MULTIPATH */
 
+#ifdef CONFIG_TREENVME
+const struct block_device_operations treenvme_fops = {
+	.owner		= THIS_MODULE,
+	.open		= nvme_open,
+	.release	= nvme_release,
+	.ioctl		= treenvme_ioctl,
+	.compat_ioctl	= nvme_compat_ioctl,
+	.getgeo		= nvme_getgeo,
+	.pr_ops		= &nvme_pr_ops,
+};
+#endif /* CONFIG_TREENVME */
+
 static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
 {
 	unsigned long timeout =
@@ -2236,7 +2258,7 @@ int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_shutdown_ctrl);
 
-static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
+void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 		struct request_queue *q)
 {
 	bool vwc = false;
@@ -2257,6 +2279,7 @@ static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 		vwc = true;
 	blk_queue_write_cache(q, vwc, vwc);
 }
+EXPORT_SYMBOL_GPL(nvme_set_queue_limits);
 
 static int nvme_configure_timestamp(struct nvme_ctrl *ctrl)
 {
@@ -2944,7 +2967,7 @@ int nvme_init_identify(struct nvme_ctrl *ctrl)
 	ret = nvme_configure_apst(ctrl);
 	if (ret < 0)
 		return ret;
-	
+
 	ret = nvme_configure_timestamp(ctrl);
 	if (ret < 0)
 		return ret;
@@ -3409,6 +3432,69 @@ static int __nvme_check_ids(struct nvme_subsystem *subsys,
 	return 0;
 }
 
+/*
+// alter
+static struct treenvme_head *nvme_alloc_treenvme_head(struct nvme_ctrl *ctrl, 
+	unsigned nsid, struct nvme_id_ns *id, 
+	struct nvme_ns_ids *ids)
+{
+	struct treenvme_head *thead;
+	size_t size = sizeof(*thead);
+	int ret = -ENOMEM;
+
+	thead = kzalloc(size, GFP_KERNEL);
+	if (!thead)
+		goto out;
+	thead->subsys = ctrl->subsys;
+	thead->ns_id = nsid;
+	thead->ids = *ids;
+	kref_init(&thead->ref);
+
+	ret = treenvme_alloc_disk(ctrl, thead);
+	if (ret) {
+		dev_err(ctrl->device,
+			"duplicate IDs for nsid %d\n", nsid);
+		goto out_cleanup_srcu;
+	}
+
+	list_add_tail(&thead->entry, &ctrl->subsys->nsheads);
+
+	kref_get(&ctrl->subsys->ref);
+	return thead;
+out_cleanup_srcu:
+	cleanup_srcu_struct(&thead->srcu);
+out_ida_remove:
+	ida_simple_remove(&ctrl->subsys->ns_ida, thead->instance);
+out_free_head:
+	kfree(thead);
+out:
+	if (ret > 0)
+		ret = blk_status_to_errno(nvme_error_status(ret));
+	return ERR_PTR(ret);
+}
+//static int nvme_init_treenvme_head(struct treenvme_ns *tns, unsigned nsid,
+static int nvme_init_treenvme_head(struct nvme_ns *ns, unsigned nsid,
+		struct nvme_id_ns *id)
+{
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	bool is_shared = id->nmic & (1 << 0);
+	struct treenvme_head *thead = NULL;
+	struct nvme_ns_ids ids;
+	int ret = 0;
+
+	mutex_lock(&ctrl->subsys->lock);
+	thead = nvme_alloc_treenvme_head(ctrl, nsid, id, &ids);
+	list_add_tail(&ns->siblings, &thead->list);
+	ns->thead = thead;
+
+out_unlock:
+	mutex_unlock(&ctrl->subsys->lock);
+out:
+	if (ret > 0)
+		ret = blk_status_to_errno(nvme_error_status(ret));
+	return ret;
+}
+*/
 static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns *id,
 		struct nvme_ns_ids *ids)
@@ -3561,10 +3647,13 @@ static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
 	return 0;
 }
 
+// alter
 static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
+	struct treenvme_ns *tns;
 	struct gendisk *disk;
+	struct gendisk *treedisk;
 	struct nvme_id_ns *id;
 	char disk_name[DISK_NAME_LEN];
 	int node = ctrl->numa_node, flags = GENHD_FL_EXT_DEVT, ret;
@@ -3587,9 +3676,8 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 
 	ns->queue->queuedata = ns;
 	ns->ctrl = ctrl;
-
 	kref_init(&ns->kref);
-	ns->lba_shift = 9; /* set to a default value for 512 until disk is validated */
+	ns->lba_shift = 9; 
 
 	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
 	nvme_set_queue_limits(ctrl, ns->queue);
@@ -3598,7 +3686,7 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	if (ret)
 		goto out_free_queue;
 
-	if (id->ncap == 0)	/* no namespace (legacy quirk) */
+	if (id->ncap == 0)
 		goto out_free_id;
 
 	ret = nvme_init_ns_head(ns, nsid, id);
@@ -3606,11 +3694,13 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 		goto out_free_id;
 	nvme_setup_streams_ns(ctrl, ns);
 	nvme_set_disk_name(disk_name, ns, ctrl, &flags);
+	//treenvme_set_name(disk_name, ns, ctrl, &flags);
 
 	disk = alloc_disk_node(0, node);
 	if (!disk)
 		goto out_unlink_ns;
 
+	// alter
 	disk->fops = &nvme_fops;
 	disk->private_data = ns;
 	disk->queue = ns->queue;
@@ -3628,15 +3718,45 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 		}
 	}
 
-	down_write(&ctrl->namespaces_rwsem);
-	list_add_tail(&ns->list, &ctrl->namespaces);
-	up_write(&ctrl->namespaces_rwsem);
+	nvme_get_ctrl(ctrl);
+	device_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);
+	nvme_mpath_add_disk(ns, id);
 
+#ifdef CONFIG_TREENVME
+
+	/*	
+	// alter_tree
+	printk(KERN_ERR "Got into treenvm creation.\n");
+	ns->tqueue = blk_alloc_queue(treenvme_make_request, ctrl->numa_node);
+	ns->tqueue->queuedata = ns;
+	blk_queue_logical_block_size(ns->tqueue, 1 << ns->lba_shift);
+	nvme_set_queue_limits(ctrl, ns->tqueue);
+
+	treenvme_set_name(disk_name, ns, ctrl, &flags);
+	
+	treedisk = alloc_disk_node(0, node);
+	if (!treedisk)
+		goto out_unlink_ns;
+
+	// alter
+	treedisk->fops = &treenvme_fops;
+	treedisk->private_data = ns;
+	treedisk->queue = ns->tqueue;
+	treedisk->flags = flags;
+	memcpy(treedisk->disk_name, disk_name, DISK_NAME_LEN);
+	ns->tdisk = treedisk;
+
+	__nvme_revalidate_disk(treedisk, id);
 	nvme_get_ctrl(ctrl);
+	device_add_disk(ctrl->device, ns->tdisk, nvme_ns_id_attr_groups);
+	*/
+	add_treedisk(ctrl, ns, nsid);
+#endif
 
-	device_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);
+	down_write(&ctrl->namespaces_rwsem);
+	list_add_tail(&ns->list, &ctrl->namespaces);
+	up_write(&ctrl->namespaces_rwsem);
 
-	nvme_mpath_add_disk(ns, id);
 	nvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);
 	kfree(id);
 
diff --git a/drivers/nvme/host/depthpath.c b/drivers/nvme/host/depthpath.c
new file mode 100644
index 000000000000..2146ac5b13e6
--- /dev/null
+++ b/drivers/nvme/host/depthpath.c
@@ -0,0 +1,1006 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2020 Yu Jian
+ */
+
+#include <linux/anon_inodes.h>
+#include <linux/file.h>
+#include <linux/moduleparam.h>
+#include <linux/module.h>
+#include <linux/blkdev.h>
+#include <linux/treenvme.h>
+#include <linux/nvme_ioctl.h>
+#include <linux/treenvme_ioctl.h>
+#include <trace/events/block.h>
+#include "nvme.h"
+#include "tokuspec.h"
+
+#define DEBUG 1
+//#define DEBUGMAX 0
+
+// Hardcoded magic variables
+#define TREENVME_OFF_BLOCKTABLE 0ULL
+#define TREENVME_OFF_SQES	0x8000000ULL
+
+static bool depthpath = true;
+module_param(depthpath, bool, 0444);
+MODULE_PARM_DESC(depthpath,
+	"turn on native support for per subsystem");
+
+static int depthcount = 4;
+module_param(depthcount, int, 0644);
+MODULE_PARM_DESC(depthcount, "number of rebound in the backpath");
+
+struct nvme_dev;
+struct nvme_completion;
+struct block_translation_pair;
+struct block_table;
+struct pivot_bounds;
+struct DBT;
+
+struct treenvme_ctx {
+	struct nvme_dev *dev;
+	struct block_table *bt;
+	struct task_struct *task;
+};
+
+static struct treenvme_ctx *tctx;
+static const struct file_operations treenvme_ctrl_fops;
+static struct kmem_cache *node_cachep; 
+static int page_match(char *page, int page_size);
+static int treenvme_setup_ctx(struct nvme_ns *ns, void *argp);
+
+void treenvme_set_name(char *disk_name, struct nvme_ns *ns, struct nvme_ctrl *ctrl, int *flags)
+{
+	sprintf(disk_name, "treenvme%d", ctrl->subsys->instance);
+}
+
+blk_qc_t treenvme_make_request(struct request_queue *q, struct bio *bio)
+{
+	struct nvme_ns *ns = q->queuedata;
+	struct device *dev = disk_to_dev(ns->tdisk);
+
+	blk_qc_t ret = BLK_QC_T_NONE;
+	int srcu_idx;
+
+	blk_queue_split(q, &bio);
+	bio->bi_disk = ns->disk;
+	bio->bi_opf |= REQ_TREENVME;
+	ret = direct_make_request(bio);
+
+	return ret;
+}
+
+// taken from io_uring
+static void *treenvme_validate_mmap_request(struct file *file, loff_t pgoff, size_t sz)
+{
+	struct treenvme_ctx *_tctx = file->private_data;
+	loff_t offset = pgoff << PAGE_SHIFT;
+	struct page *page;
+	void *ptr;
+
+	switch(offset) {
+	case TREENVME_OFF_BLOCKTABLE:
+#ifdef DEBUG
+		printk(KERN_ERR "Mmap'ed at the block table.\n");
+#endif
+		ptr = _tctx->bt;
+		break;
+	case TREENVME_OFF_SQES:
+#ifdef DEBUG
+		printk(KERN_ERR "Mmap'ed at the submission events.\n");
+#endif
+		ptr = _tctx->bt;
+		break;
+	default:
+		return ERR_PTR(-EINVAL);
+	}	
+
+	page = virt_to_head_page(ptr);
+	if (sz > page_size(page))
+		return ERR_PTR(-EINVAL);
+
+	return ptr;
+}
+
+static int treenvme_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	size_t sz = vma->vm_end - vma->vm_start;
+	unsigned long pfn;
+	void *ptr;
+
+	ptr = treenvme_validate_mmap_request(file, vma->vm_pgoff, sz);
+	if (IS_ERR(ptr))
+		return PTR_ERR(ptr);
+
+	pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
+
+}
+/*
+int treenvme_alloc_disk(struct nvme_ctrl *ctrl, struct treenvme_head *thead)
+{
+	printk(KERN_ERR "Alloc'ing disk\n");
+	struct request_queue *q;
+	bool wbc = false;
+
+	mutex_init(&thead->lock);
+	//INIT_WORK(&thead->requeue_work, nvme_requeue_work);
+
+	q = blk_alloc_queue(treenvme_make_request, ctrl->numa_node);
+	if (!q)
+		goto out;
+	q->queuedata = thead;
+	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
+	blk_queue_logical_block_size(q, 4096);
+
+	blk_queue_write_cache(q, wbc, wbc);
+
+	thead->disk = alloc_disk(0);
+	thead->disk->fops = &treenvme_fops;
+	thead->disk->private_data = thead;
+	thead->disk->queue = q;
+	thead->disk->flags = GENHD_FL_EXT_DEVT;
+
+	sprintf(thead->disk->disk_name, "treenvmen%d", thead->instance);
+	return 0;
+
+out:
+	return -ENOMEM;
+}
+*/
+
+/*
+int treenvme_resubmit_path(struct nvme_queue *nvmeq, struct request *rq, u16 idx)
+{
+	volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
+}
+*/
+
+inline void add_treedisk(struct nvme_ctrl *ctrl, struct nvme_ns *ns, unsigned nsid) {
+	struct gendisk *treedisk;
+	char disk_name[DISK_NAME_LEN];
+	struct nvme_id_ns *id;
+	int node = ctrl->numa_node, flags = GENHD_FL_EXT_DEVT;
+	int ret;
+
+	ret = nvme_identify_ns(ctrl, nsid, &id);
+
+	printk(KERN_ERR "Got into treenvme creation. \n");
+	ns->tqueue = blk_alloc_queue(treenvme_make_request, ctrl->numa_node);
+	ns->tqueue->queuedata = ns;
+	blk_queue_logical_block_size(ns->tqueue, 1 << ns->lba_shift);
+	nvme_set_queue_limits(ctrl, ns->tqueue);
+
+	treenvme_set_name(disk_name, ns, ctrl, &flags);
+	treedisk = alloc_disk_node(0, node);
+	
+	treedisk->fops = &treenvme_fops;
+	treedisk->private_data = ns;
+	treedisk->queue = ns->tqueue;
+	treedisk->flags = flags;
+	memcpy(treedisk->disk_name, disk_name, DISK_NAME_LEN);
+	ns->tdisk = treedisk;
+
+	__nvme_revalidate_disk(treedisk, id);
+	nvme_get_ctrl(ctrl);
+	printk(KERN_ERR "Disk name added is: %s\n", disk_name);
+	device_add_disk(ctrl->device, ns->tdisk, nvme_ns_id_attr_groups);	
+
+}	
+
+static int treenvme_get_fd(struct treenvme_ctx *tctx)
+{
+	struct file *file;
+	int ret;
+
+	ret = get_unused_fd_flags( O_RDWR | O_CLOEXEC );
+	if (ret < 0)
+		goto err; 
+	file = anon_inode_getfile("[treenvme]", &treenvme_ctrl_fops, tctx, O_RDWR | O_CLOEXEC);
+	if (IS_ERR(file)){
+		put_unused_fd(ret);
+		ret = PTR_ERR(file);
+		goto err;
+	}
+err:
+	return -1;
+}
+// setup
+static int treenvme_setup_ctx(struct nvme_ns *ns, void *argp) 
+{
+	int r;
+#ifdef DEBUG
+	printk(KERN_ERR "Got into treenvme context setup.\n");
+#endif
+	struct nvme_ns *file;
+	tctx->task = get_task_struct(current);	
+	
+	r = treenvme_get_fd(tctx);
+#ifdef DEBUG
+	printk(KERN_ERR "File is %u\n", r);
+#endif
+	return r;
+}
+
+// All the treenvme operations
+
+static void *treenvme_add_user_metadata(struct bio *bio, void __user *ubuf,
+		unsigned len, u32 seed, bool write)
+{
+	struct bio_integrity_payload *bip;
+	int ret = -ENOMEM;
+	void *buf;
+
+	buf = kmalloc(len, GFP_KERNEL);
+	if (!buf)
+		goto out;
+
+	ret = -EFAULT;
+	if (write && copy_from_user(buf, ubuf, len))
+		goto out_free_meta;
+
+	bip = bio_integrity_alloc(bio, GFP_KERNEL, 1);
+	if (IS_ERR(bip)) {
+		ret = PTR_ERR(bip);
+		goto out_free_meta;
+	}
+
+	bip->bip_iter.bi_size = len;
+	bip->bip_iter.bi_sector = seed;
+	ret = bio_integrity_add_page(bio, virt_to_page(buf), len,
+			offset_in_page(buf));
+	if (ret == len)
+		return buf;
+	ret = -ENOMEM;
+out_free_meta:
+	kfree(buf);
+out:
+	return ERR_PTR(ret);
+}
+
+static int treenvme_submit_user_cmd(struct request_queue *q,
+		struct nvme_command *cmd, void __user *ubuffer,
+		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
+		u32 meta_seed, u64 *result, unsigned timeout)
+{
+	bool write = nvme_is_write(cmd);
+	struct nvme_ns *ns = q->queuedata;
+	struct gendisk *disk = ns ? ns->disk : NULL;
+	struct request *req;
+	struct bio *bio = NULL;
+	void *meta = NULL;
+	int ret;
+
+	req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
+	if (IS_ERR(req))
+		return PTR_ERR(req);
+
+	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+	nvme_req(req)->flags |= NVME_REQ_USERCMD;
+
+	if (ubuffer && bufflen) {
+		ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
+				GFP_KERNEL);
+		if (ret)
+			goto out;
+		bio = req->bio;
+		bio->bi_disk = disk;
+		if (disk && meta_buffer && meta_len) {
+			meta = treenvme_add_user_metadata(bio, meta_buffer, meta_len,
+					meta_seed, write);
+			if (IS_ERR(meta)) {
+				ret = PTR_ERR(meta);
+				goto out_unmap;
+			}
+			req->cmd_flags |= REQ_INTEGRITY;
+		}
+	}
+
+	blk_execute_rq(req->q, disk, req, 0);
+	if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
+		ret = -EINTR;
+	else
+		ret = nvme_req(req)->status;
+	if (result)
+		*result = le64_to_cpu(nvme_req(req)->result.u64);
+	if (meta && !ret && !write) {
+		if (copy_to_user(meta_buffer, meta, meta_len))
+			ret = -EFAULT;
+	}
+	kfree(meta);
+ out_unmap:
+	if (bio)
+		blk_rq_unmap_user(bio);
+ out:
+	blk_mq_free_request(req);
+	return ret;
+}
+
+static void __user *nvme_to_user_ptr(uintptr_t ptrval)
+{
+	if (in_compat_syscall())
+		ptrval = (compat_uptr_t)ptrval;
+	return (void __user *)ptrval;
+}
+
+static int treenvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
+{
+	struct nvme_user_io io;
+	struct nvme_command c;
+	unsigned length, meta_len;
+	void __user *metadata;
+
+	if (copy_from_user(&io, uio, sizeof(io)))
+		return -EFAULT;
+	if (io.flags)
+		return -EINVAL;
+
+	switch (io.opcode) {
+	case nvme_cmd_write:
+	case nvme_cmd_read:
+	case nvme_cmd_compare:
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	length = (io.nblocks + 1) << ns->lba_shift;
+	meta_len = (io.nblocks + 1) * ns->ms;
+	metadata = nvme_to_user_ptr(io.metadata);
+
+	if (ns->ext) {
+		length += meta_len;
+		meta_len = 0;
+	} else if (meta_len) {
+		if ((io.metadata & 3) || !io.metadata)
+			return -EINVAL;
+	}
+
+	memset(&c, 0, sizeof(c));
+	c.rw.opcode = io.opcode;
+	c.rw.flags = io.flags;
+	c.rw.nsid = cpu_to_le32(ns->head->ns_id);
+	c.rw.slba = cpu_to_le64(io.slba);
+	c.rw.length = cpu_to_le16(io.nblocks);
+	c.rw.control = cpu_to_le16(io.control);
+	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
+	c.rw.reftag = cpu_to_le32(io.reftag);
+	c.rw.apptag = cpu_to_le16(io.apptag);
+	c.rw.appmask = cpu_to_le16(io.appmask);
+
+	return treenvme_submit_user_cmd(ns->queue, &c,
+			nvme_to_user_ptr(io.addr), length,
+			metadata, meta_len, lower_32_bits(io.slba), NULL, 0);
+}
+
+static void nvme_put_ns_from_disk(struct nvme_ns_head *head, int idx)
+{
+	if (head)
+		srcu_read_unlock(&head->srcu, idx);
+}
+
+// blocktable
+static int register_block_table(struct nvme_ns *ns, struct block_table *bt)
+{
+	tctx->bt->length_of_array = bt->length_of_array;
+	tctx->bt->smallest = bt->smallest;
+	tctx->bt->next_head = bt->next_head;
+	
+#ifdef DEBUG
+	printk(KERN_ERR "Length of array is: %llu \n", tctx->bt->length_of_array);
+	printk(KERN_ERR "Smallest element is: %u \n", tctx->bt->smallest);
+	printk(KERN_ERR "Next head is: %u \n", tctx->bt->next_head);	
+#endif
+#ifdef DEBUG
+	int i = 0;
+	printk(KERN_ERR "PRINTING WHOLE BLOCK TABLE.\n");
+#endif
+
+	void * user_ptr;
+	user_ptr = bt->block_translation;
+	printk(KERN_ERR "USERSPACE ADDRESS IS %u.\n", user_ptr);
+	tctx->bt->block_translation = kmalloc((sizeof (struct block_translation_pair)) * bt->length_of_array, GFP_KERNEL);
+	copy_from_user(tctx->bt->block_translation, user_ptr, sizeof(struct block_translation_pair) * bt->length_of_array);
+
+#ifdef DEBUG
+	printk(KERN_ERR "Finish transferring.\n");
+	for (i = 0; i < bt->length_of_array; i++)
+	{
+		printk(KERN_ERR "For blocknum %u", i);
+		printk(KERN_ERR "OFFSET: %llu", tctx->bt->block_translation[i].u.diskoff);
+		printk(KERN_ERR "SIZE: %llu", tctx->bt->block_translation[i].size);
+	}
+#endif
+}
+
+int treenvme_ioctl(struct block_device *bdev, fmode_t mode,
+		unsigned int cmd, unsigned long arg)
+{
+	struct nvme_ns_head *head = NULL;
+	void __user *argp = (void __user *)arg;
+	struct nvme_ns *ns;
+	int srcu_idx, ret;
+
+	ns = bdev->bd_disk->private_data;
+	if (unlikely(!ns))
+		return -EWOULDBLOCK;
+
+	/*
+	 * Handle ioctls that apply to the controller instead of the namespace
+	 * seperately and drop the ns SRCU reference early.  This avoids a
+	 * deadlock when deleting namespaces using the passthrough interface.
+	 */
+	
+	/*
+	 * if (is_ctrl_ioctl(cmd))
+		return nvme_handle_ctrl_ioctl(ns, cmd, argp, head, srcu_idx);
+	*/
+#ifdef DEBUG
+	printk(KERN_ERR "Got into treenvme IOCTL.\n");
+#endif
+
+	switch (cmd) {
+	/*
+	case NVME_IOCTL_ID:
+		force_successful_syscall_return();
+		ret = ns->head->ns_id;
+		break;
+	*/
+	case TREENVME_IOCTL_IO_CMD:
+#ifdef DEBUG
+		printk(KERN_ERR "Submitted IO CMD through IOCTL.\n");
+#endif
+		//ret = nvme_user_cmd(ns->ctrl, ns, argp);
+		break;
+	case TREENVME_IOCTL_SUBMIT_IO:
+#ifdef DEBUG
+		printk(KERN_ERR "Submit IO through IOCTL process.\n");
+#endif
+		//ret = treenvme_submit_io(ns, argp);
+		break;
+	/*
+	case NVME_IOCTL_IO64_CMD:
+		ret = nvme_user_cmd64(ns->ctrl, ns, argp);
+		break;
+	*/
+	case TREENVME_IOCTL_SETUP:
+#ifdef DEBUG
+		printk(KERN_ERR "Setup nvme ioctl process.\n");
+#endif
+		ret = treenvme_setup_ctx(ns, argp);
+		break;
+	case TREENVME_IOCTL_REGISTER_BLOCKTABLE:
+#ifdef DEBUG
+		printk(KERN_ERR "Attempt to register blocktable. \n");
+#endif
+		ret = register_block_table(ns, argp);
+		break;
+	default:
+		if (ns->ndev)
+			ret = nvme_nvm_ioctl(ns, cmd, arg);
+		else
+			ret = -ENOTTY;
+	}
+
+	nvme_put_ns_from_disk(head, srcu_idx);
+	return ret;
+}
+
+// end
+static inline struct blk_mq_tags *nvme_queue_tagset(struct nvme_queue *nvmeq)
+{
+	if (!nvmeq->qid)
+		return nvmeq->dev->admin_tagset.tags[0];
+	return nvmeq->dev->tagset.tags[nvmeq->qid - 1];
+}
+
+inline void nvme_backpath(struct nvme_queue *nvmeq, u16 idx, struct request *req, struct nvme_completion *cqe)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	//struct nvme_queue *nvmeq = iod->nvmeq;
+	struct nvme_ns *ns = req->q->queuedata;
+	struct nvme_dev *dev = iod->nvmeq->dev;
+	struct nvme_command cmnd;
+	blk_status_t ret;
+
+	//printk(KERN_ERR "GOT HERE -- rebound \n");
+	if (req->alter_count < depthcount && !op_is_write(req_op(req)))
+	{
+		req->alter_count += 1;
+		// alter
+		ret = nvme_setup_cmd(ns, req, &cmnd);
+		if (ret)
+			printk(KERN_ERR "submit error\n");
+		//printk(KERN_ERR "Got here 2\n");x
+		if (blk_rq_nr_phys_segments(req)) {
+			ret = nvme_map_data(dev, req, &cmnd);
+			if (ret)
+				printk(KERN_ERR "mapping error\n");
+		}
+		if (blk_integrity_rq(req)) {
+			ret = nvme_map_metadata(dev, req, &cmnd);
+			if (ret)
+				printk(KERN_ERR "meta error\n");
+		}
+		cmnd.rw.slba = cpu_to_le64(nvme_sect_to_lba(ns, blk_rq_pos(req)));
+		//printk(KERN_ERR "SECTOR NUMBER IS %u\n", cmnd.rw.slba);
+
+		int ret;
+		struct bio_vec bvec;
+		struct req_iterator iter;
+
+		rq_for_each_segment(bvec, req, iter)
+		{
+			char *buffer = bio_data(req->bio);
+#ifdef DEBUG
+			printk(KERN_ERR "char bio: %s \n", buffer);
+			printk(KERN_ERR "char is: %c\n", buffer[2]);	
+			printk(KERN_ERR "size is: %u\n", req->bio->bi_iter.bi_size);
+#endif
+			// retry
+			int next_page;
+			next_page = page_match(buffer, 4096);
+			if (next_page == 0)
+				goto ERROR;
+			if (!tctx->bt || !tctx->bt->block_translation)
+			{
+				printk(KERN_ERR "No block table when we want to do lookup.\n");
+				goto ERROR;
+			}
+			if (next_page > tctx->bt->length_of_array) {
+				printk(KERN_ERR "Does not fit!\n");
+				goto ERROR;
+			}
+#ifdef DEBUG
+			printk(KERN_ERR "NEXT PAGE IS %u\n", next_page);
+			printk(KERN_ERR "Length of array is: %llu \n", tctx->bt->length_of_array);
+			printk(KERN_ERR "Smallest element is: %u \n", tctx->bt->smallest);
+			printk(KERN_ERR "Next head is: %u \n", tctx->bt->next_head);	
+			int i = 0;
+			for (i = 0; i < tctx->bt->length_of_array; i++)
+			{
+				printk(KERN_ERR "For blocknum %u", i);
+				printk(KERN_ERR "OFFSET: %llu", tctx->bt->block_translation[i].u.diskoff);
+				printk(KERN_ERR "SIZE: %llu", tctx->bt->block_translation[i].size);
+			}
+#endif
+			uint64_t next_offset;
+			next_offset = tctx->bt->block_translation[next_page].u.diskoff;
+
+#ifdef DEBUG
+			printk(KERN_ERR "The next offset is %llu\n", next_offset);
+#endif
+			// cmnd.rw.slba = cpu_to_le64(nvme_lba_to_sect(ns, next_offset));
+			cmnd.rw.slba = cpu_to_le64(next_offset / 512);
+			req->__sector = cmnd.rw.slba;
+			/*
+			if (buffer[a] == "a"){
+				cmnd.rw.slba += cpu_to_le64(cmnd.rw.slba * 2);
+				printk(KERN_ERR "SECTOR NUMBER IS %u\n", cmnd.rw.slba);
+				printk(KERN_ERR "matched.\n");
+				req->__sector = cmnd.rw.slba;
+			}
+			*/
+		}
+		nvme_req(req)->cmd = &cmnd;
+		nvme_submit_cmd(nvmeq, &cmnd, true);
+	}
+	else
+	{
+ERROR:
+		// just some final sanity check
+		printk(KERN_ERR "Final count is %u\n", req->alter_count);
+		req = blk_mq_tag_to_rq(nvme_queue_tagset(nvmeq), req->first_command_id);
+		nvme_end_request(req, cqe->status, cqe->result);
+	}
+
+}
+
+void init_pivot(struct pivot_bounds *pb, int num) {
+	pb->num_pivots = num;
+	pb->total_size = 0;
+	pb->fixed_keys = NULL;
+	pb->fixed_keylen_aligned = 0;
+	//pb->dbt_keys = NULL;
+	pb->dbt_keys = kmalloc(sizeof(struct DBT) * pb->num_pivots, GFP_KERNEL);
+}
+
+static void init_DBT(struct DBT *new)
+{
+	memset(new, 0, sizeof(*new));
+	return new;
+}
+
+static int compare (struct search_ctx *srch, struct DBT *keya, struct DBT *keyb)
+{
+	char *keyadata = keya->data;
+	char *keybdata = keyb->data;
+	if (srch->compare(keyadata, keybdata, keya->size, keyb->size))
+	{
+		return 0;
+	}
+	else {
+		return 1;
+	}
+}
+int fill_pivot(struct pivot_bounds *pb, char *page, int n)
+{
+	int k = 0;
+	int i = 0;
+
+	pb->num_pivots = n;
+	pb->total_size = 0;
+	pb->fixed_keys = NULL;
+	pb->fixed_keylen = 0;
+	pb->dbt_keys = NULL;
+
+	pb->dbt_keys = kmalloc(sizeof(struct DBT) * pb->num_pivots, GFP_KERNEL);
+	for (i = 0; i < n; i++) {
+		uint32_t size;
+		memcpy(&size, &page[k], 4);
+		k += 4;
+		memcpy(&pb->dbt_keys[i].data, &page[k], size);
+#ifdef DEBUG
+	printk("Size is %u\n", size);
+	printk("Data is %u\n", pb->dbt_keys[i].data);
+#endif
+		pb->total_size += size;
+	       	k += size;	
+	}
+	return k;
+}
+
+static int deserialize_basement(char *page, struct child_node_data *cnd) 
+{
+	int i = 0;
+	uint32_t num_entries = 0;
+	uint32_t key_data_size = 0;
+	uint32_t val_data_size = 0;
+	uint32_t fixed_klpair_length = 0;
+	bool all_keys_same_len = false;
+	bool key_vals_sep = false;
+
+#ifdef DEBUG
+	int z = 0;
+	for (z = 0; z < 512; z++){
+		if (page[z] != 0)
+		{
+			printk(KERN_ERR "we have %u @ %u", page[z], z);
+		}	
+	}
+#endif
+	i += 4;
+
+	// starting offset should be 4 i think
+	memcpy(&num_entries, &page[i], 4);
+	i += 8;
+		
+#ifdef DEBUG
+	printk(KERN_ERR "page val: %u\n", page[0]);
+#endif
+
+/*
+	memcpy(&key_data_size, &page[i], 4);
+	i += 8;
+
+	memcpy(&val_data_size, &page[i], 4);
+	i += 8;
+
+	memcpy(&fixed_klpair_length, &page[i], 4);
+	i += 8;
+
+	memcpy(&all_keys_same_len, &page[i], 1);
+	i += 8;
+
+	memcpy(&key_vals_sep, &page[i], 1);
+	i += 8;
+
+#ifdef DEBUG
+	printk(KERN_ERR "NUM_ENTRIES: %u\n", num_entries);
+	printk(KERN_ERR "KEY_DATA_SIZE: %u\n", key_data_size);
+	printk(KERN_ERR "VAL_DATA_SIZE: %u\n", val_data_size);
+	printk(KERN_ERR "KLPAIR_LEN: %u\n", fixed_klpair_length);
+#endif
+
+	char *bytes = kmalloc(sizeof(char) * key_data_size, GFP_KERNEL);
+	memcpy(&bytes, &page[i], key_data_size);
+	i += key_data_size;
+*/
+}
+
+// Reference: toku_deserialize_bp_from_disk
+static int deserialize(char *page, struct tokunode *node) 
+{
+	struct block_data *bd;
+	// node->blocknum = blocknum;
+	node->blocknum = 0;
+	node->ct_pair = NULL;
+	int z = 0;
+
+#ifdef DEBUGMAX	
+	for (z = 0; z < 512; z++){
+		if (page[z] != 0)
+		{
+			printk(KERN_ERR "we have %u @ %u", page[z], z);
+		}	
+	}
+#endif
+
+	int i = 0;
+	int j; 
+	int k;
+	int l;
+	int m = 0;
+
+	char buffer[8];
+	memcpy(buffer, &page[i], 8);
+       	i += 8;
+	if (memcmp(buffer, "tokuleaf", 8) != 0 
+	   && memcmp(buffer, "tokunode", 8) != 0)
+	{
+		printk(KERN_WARNING "No leaf word in buffer.\n");
+		goto ERROR;
+	}
+#ifdef DEBUG
+	printk(KERN_ERR "BUFFER: %.*s\n", 8, buffer);
+#endif
+
+	uint32_t version;
+	memcpy(&version, &page[i], 4);
+	i += 4;
+
+#ifdef DEBUG
+	printk(KERN_ERR "VERSION_NUM: %u\n", version);
+#endif
+
+	i += 8; // skip layout version and build id
+	
+	uint32_t num_child;
+	memcpy(&num_child, &page[i], 4);
+	i += 4;
+	node->n_children = num_child;
+
+#ifdef DEBUG
+	printk(KERN_ERR "NUM_CHILD: %u\n", num_child);
+#endif
+
+	// node->bp = kmalloc(sizeof(struct ftnode_partition) * num_child, GFP_KERNEL);
+	bd = kmalloc(sizeof(struct block_data) * num_child, GFP_KERNEL);
+
+	for (j = 0; j < node->n_children; j++) {
+		//printk("PAGE HERE IS: %u\n", page[i]);
+		memcpy(&bd[j].start, &page[i], 4);
+		i += 4;
+		memcpy(&bd[j].size, &page[i], 4);
+		i += 4;
+		bd[j].end = bd[j].start + bd[j].size;
+#ifdef DEBUG
+	printk("CHILD_NUM: %u\n", j);
+	printk("BLOCK_START: %u\n", bd[j].start);
+	printk("BLOCK_END: %u\n", bd[j].end);	
+	printk("BLOCK_SIZE: %u\n", bd[j].size); 
+#endif
+	}
+
+	i += 4; // skip checksumming
+
+	struct subblock_data *sb_data = kmalloc(sizeof(struct subblock_data), GFP_KERNEL);
+	sb_data->csize = 0;
+	sb_data->usize = 0;
+	
+	// compressed size
+	memcpy(&sb_data->csize, &page[i], 4);
+	i += 4;
+
+	// uncompressed size
+	memcpy(&sb_data->usize, &page[i], 4);
+	i += 4;	
+	/*
+	for (j = 0; i < 400; j++) {
+		memcpy(&sb_data.usize, &page[i], 4);
+		i += 4;
+	}
+	*/
+
+#ifdef DEBUG
+	printk("COMPRESSED_SIZE: %u\n", sb_data->csize);
+	printk("UNCOMPRESSED_SIZE: %u\n", sb_data->usize);
+	printk("COUNTER IS AT: %u\n", i);
+#endif
+	
+	// skip compressing
+
+	char *cp = kmalloc(sizeof(int) * sb_data->csize, GFP_KERNEL);
+	memcpy(cp, &page[i], sb_data->csize);
+	
+	// decompress by moving everything one to the left
+	char *temp = kmalloc(sizeof(int) * sb_data->usize, GFP_KERNEL);
+	memcpy(temp, cp + 1, sb_data->csize - 1);
+
+	kfree(cp);
+	cp = temp;
+	//memcpy(&cp, &page[i], 4);
+
+	// get from subblock_data
+	uint32_t data_size = 0;
+	if (sb_data->usize != 0) 
+		data_size = sb_data->usize - 4;
+
+#ifdef DEBUG 
+	printk("DATA_SIZE: %u\n", data_size);
+#endif
+
+#ifdef DEBUG	
+	for (z = 0; z < data_size; z++){
+		if (cp[z] != 0)
+		{
+			printk(KERN_ERR "we have %u @ %u in subblock", cp[z], z);
+		}	
+	}
+#endif
+	if (data_size != 0) 
+	{
+		char bufferd[data_size];
+		memcpy(&bufferd, &cp[m], data_size);
+		m += data_size;
+
+		k = 0;
+		k += 12;
+		memcpy(&node->flags, &bufferd[k], 4);
+
+		k += 4;
+		memcpy(&node->height, &bufferd[k], 4);
+
+#ifdef DEBUG
+	printk("Node flags of %u\n", node->flags);
+	printk("Node height of %u\n", node->height);
+#endif
+
+		k += 12;
+		node->pivotkeys = kmalloc(sizeof(struct pivot_bounds), GFP_KERNEL); 
+		if (node->n_children > 1){
+			k += fill_pivot(node->pivotkeys, &bufferd[k], node->n_children); 
+		}	
+		else {
+			init_pivot(node->pivotkeys, 0);
+		}
+
+		// puts into node stuff
+		node->cnd = kmalloc(sizeof(struct child_node_data) * node->n_children, GFP_KERNEL);
+
+		// Block nums
+		if (node->height > 0) {
+			for (l = 0; l < node->n_children; l++) {
+				memcpy(&node->cnd[l].blocknum, &bufferd[k], 4);
+				k += 8;
+#ifdef DEBUG
+	printk("CHILD_BLOCKNUM: %d\n", node->cnd[l].blocknum);
+#endif			
+			}
+		}
+		else {
+			for (l = 0; l < node->n_children; l++) {
+				memcpy(&node->cnd[l].blocknum, &bufferd[k], 4);
+				k += 8;
+#ifdef DEBUG
+	printk("LEAF_CHILD_BLOCKNUM: %d\n", node->cnd[l].blocknum);
+#endif			
+				deserialize_basement(&bufferd[k], &node->cnd[l]);
+			}
+			return 0;
+		}
+
+		return 1; 
+	}
+	else {
+		k += 16;
+		goto ERROR;
+	}
+ERROR:
+	return -1;	
+}
+
+// example function -- don't use
+static int example_compare(char *a, char *b, int asize, int bsize)
+{
+	int i = 0;
+	int j = 0;
+
+	for (i = 0; i < asize; i++) {
+		for (j = 0; j < bsize; j++) {
+			if (a[i] > b[j])
+				return i;
+		}
+	}
+	return -1;	
+}
+
+static int page_match(char *page, int page_size)
+{
+	struct tokunode *node = kmem_cache_alloc(node_cachep, GFP_KERNEL);
+
+	int result;
+	result = deserialize(page, node);
+	if (result == -1)
+		return -1;
+
+	int low = 0;
+	int high = node->n_children - 1;
+	int middle;
+	struct DBT pivot_key;
+	init_DBT(&pivot_key);
+
+	struct search_ctx *search = kmalloc(sizeof(struct search_ctx), GFP_KERNEL);
+	
+	// this is only a test?
+	search->compare = &example_compare;
+	while (low < high)
+	{
+		middle = (low + high) / 2;	
+		bool c = compare(search, &node->pivotkeys->dbt_keys[low], &node->pivotkeys->dbt_keys[high]);
+		if (((search->direction == LEFT_TO_RIGHT) && c) || (search->direction == RIGHT_TO_LEFT && !c))
+		{	
+			high = middle;
+		}
+		else {
+			low = middle + 1;
+		}
+		break;		
+	}
+#ifdef DEBUG
+	if (!node)
+		printk(KERN_ERR "Node is NULL\n");
+#endif
+
+	if (result == 1)
+	{
+		return (node)->cnd[low].blocknum;	
+	}
+	if (result == 0)
+	{
+		return 0;
+	}
+}
+
+static int __init treenvme_init(void)
+{
+#ifdef DEBUG
+	printk(KERN_ERR "Got into original treenvme init. \n");
+#endif
+	tctx = kmalloc(sizeof(struct treenvme_ctx), GFP_KERNEL);
+	tctx->bt = kmalloc(sizeof(struct block_table), GFP_KERNEL);	
+	
+	// this is how we keep the nodes in memory
+	node_cachep = KMEM_CACHE(tokunode, SLAB_HWCACHE_ALIGN | SLAB_PANIC);	
+
+	DECLARE_HASHTABLE(tbl, 4);
+}
+
+static void __exit treenvme_exit(void)
+{
+	kfree(tctx);
+}
+
+static const struct file_operations treenvme_ctrl_fops = {
+	.owner		= THIS_MODULE,
+	.mmap		= treenvme_mmap,
+// probably have to do release and flush
+};
+
+/*
+const struct block_device_operations treenvme_fops = {
+	.owner		= THIS_MODULE,
+	.open		= nvme_open,
+	.release	= nvme_release,
+	.ioctl		= treenvme_ioctl,
+	.compat_ioctl	= nvme_compat_ioctl,
+	.getgeo		= nvme_getgeo,
+	.pr_ops		= &nvme_pr_ops,
+};
+*/
+
+MODULE_AUTHOR("Yu Jian <yujian.wu1@gmail.com>");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("1.0");
+module_init(treenvme_init);
+module_exit(treenvme_exit);
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 2e04a36296d9..9689fa7b87bf 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -38,6 +38,8 @@ extern struct workqueue_struct *nvme_wq;
 extern struct workqueue_struct *nvme_reset_wq;
 extern struct workqueue_struct *nvme_delete_wq;
 
+//extern struct nvme_queue;
+
 enum {
 	NVME_NS_LBA		= 0,
 	NVME_NS_LIGHTNVM	= 1,
@@ -146,6 +148,8 @@ struct nvme_request {
  */
 #define REQ_NVME_MPATH		REQ_DRV
 
+#define REQ_TREENVME		REQ_SWAP
+
 enum {
 	NVME_REQ_CANCELLED		= (1 << 0),
 	NVME_REQ_USERCMD		= (1 << 1),
@@ -353,6 +357,8 @@ struct nvme_ns_head {
 	struct list_head	entry;
 	struct kref		ref;
 	int			instance;
+	struct treenvme_ns *tns;
+
 #ifdef CONFIG_NVME_MULTIPATH
 	struct gendisk		*disk;
 	struct bio_list		requeue_list;
@@ -368,7 +374,9 @@ struct nvme_ns {
 
 	struct nvme_ctrl *ctrl;
 	struct request_queue *queue;
+	struct request_queue *tqueue;
 	struct gendisk *disk;
+	struct gendisk *tdisk;
 #ifdef CONFIG_NVME_MULTIPATH
 	enum nvme_ana_state ana_state;
 	u32 ana_grpid;
@@ -377,6 +385,59 @@ struct nvme_ns {
 	struct nvm_dev *ndev;
 	struct kref kref;
 	struct nvme_ns_head *head;
+	struct treenvme_head *thead;
+
+	int lba_shift;
+	u16 ms;
+	u16 sgs;
+	u32 sws;
+	bool ext;
+	u8 pi_type;
+	unsigned long flags;
+#define NVME_NS_REMOVING	0
+#define NVME_NS_DEAD     	1
+#define NVME_NS_ANA_PENDING	2
+	u16 noiob;
+
+	struct nvme_fault_inject fault_inject;
+
+};
+
+// alter
+struct treenvme_head {
+	struct list_head	list;
+	struct srcu_struct      srcu;
+	struct nvme_subsystem	*subsys;
+	unsigned		ns_id;
+	struct nvme_ns_ids	ids;
+	struct list_head	entry;
+	struct kref		ref;
+	int			instance;
+	struct gendisk		*disk;
+	struct bio_list		requeue_list;
+	spinlock_t		requeue_lock;
+	struct work_struct	requeue_work;
+	struct mutex		lock;
+#ifdef CONFIG_TREENVME
+	struct nvme_ns __rcu	*current_path[];
+#endif
+};
+
+struct treenvme_ns {
+	struct list_head list;
+	struct nvme_ctrl *ctrl;
+	struct request_queue *queue;
+	struct gendisk *disk;
+#ifdef CONFIG_TREENVME
+	enum nvme_ana_state ana_state;
+	u32 ana_grpid;
+#endif
+	struct list_head siblings;
+	struct nvm_dev *ndev;
+	struct kref kref;
+	struct nvme_ns_head *head;
+	// alter
+	struct treenvme_head *thead;
 
 	int lba_shift;
 	u16 ms;
@@ -540,6 +601,128 @@ int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
 extern const struct attribute_group *nvme_ns_id_attr_groups[];
 extern const struct block_device_operations nvme_ns_head_ops;
 
+// alter -- treenvme
+extern const struct block_device_operations treenvme_fops;
+/*
+ * Represents an NVM Express device.  Each nvme_dev is a PCI function.
+ */
+struct nvme_dev {
+	struct nvme_queue *queues;
+	struct blk_mq_tag_set tagset;
+	struct blk_mq_tag_set admin_tagset;
+	u32 __iomem *dbs;
+	struct device *dev;
+	struct dma_pool *prp_page_pool;
+	struct dma_pool *prp_small_pool;
+	unsigned online_queues;
+	unsigned max_qid;
+	unsigned io_queues[HCTX_MAX_TYPES];
+	unsigned int num_vecs;
+	int q_depth;
+	int io_sqes;
+	u32 db_stride;
+	void __iomem *bar;
+	unsigned long bar_mapped_size;
+	struct work_struct remove_work;
+	struct mutex shutdown_lock;
+	bool subsystem;
+	u64 cmb_size;
+	bool cmb_use_sqes;
+	u32 cmbsz;
+	u32 cmbloc;
+	struct nvme_ctrl ctrl;
+	u32 last_ps;
+
+	mempool_t *iod_mempool;
+
+	/* shadow doorbell buffer support: */
+	u32 *dbbuf_dbs;
+	dma_addr_t dbbuf_dbs_dma_addr;
+	u32 *dbbuf_eis;
+	dma_addr_t dbbuf_eis_dma_addr;
+
+	/* host memory buffer support: */
+	u64 host_mem_size;
+	u32 nr_host_mem_descs;
+	dma_addr_t host_mem_descs_dma;
+	struct nvme_host_mem_buf_desc *host_mem_descs;
+	void **host_mem_desc_bufs;
+};
+/*
+ * An NVM Express queue.  Each device has at least two (one for admin
+ * commands and one for I/O commands).
+ */
+struct nvme_queue {
+	struct nvme_dev *dev;
+	spinlock_t sq_lock;
+	void *sq_cmds;
+	 /* only used for poll queues: */
+	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
+	volatile struct nvme_completion *cqes;
+	dma_addr_t sq_dma_addr;
+	dma_addr_t cq_dma_addr;
+	u32 __iomem *q_db;
+	u16 q_depth;
+	u16 cq_vector;
+	u16 sq_tail;
+	u16 last_sq_tail;
+	u16 cq_head;
+	u16 qid;
+	u8 cq_phase;
+	u8 sqes;
+	unsigned long flags;
+#define NVMEQ_ENABLED		0
+#define NVMEQ_SQ_CMB		1
+#define NVMEQ_DELETE_ERROR	2
+#define NVMEQ_POLLED		3
+	u32 *dbbuf_sq_db;
+	u32 *dbbuf_cq_db;
+	u32 *dbbuf_sq_ei;
+	u32 *dbbuf_cq_ei;
+	struct completion delete_done;
+};
+
+/*
+ * The nvme_iod describes the data in an I/O.
+ *
+ * The sg pointer contains the list of PRP/SGL chunk allocations in addition
+ * to the actual struct scatterlist.
+ */
+struct nvme_iod {
+	struct nvme_request req;
+	struct nvme_queue *nvmeq;
+	bool use_sgl;
+       	int aborted;
+	int npages;		/* In the PRP list. 0 means small pool in use */
+	int nents;		/* Used in scatterlist */
+	dma_addr_t first_dma;
+	unsigned int dma_len;	/* length of single DMA segment mapping */
+	dma_addr_t meta_dma;
+	struct scatterlist *sg;
+};
+void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd, bool write_sq);
+blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req, struct nvme_command *cmnd);
+blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req, struct nvme_command *cmnd);
+extern const struct block_device_operations treenvme_head_ops;
+//void add_treedisk(struct nvme_ctrl *ctrl, struct nvme_ns *ns, unsigned nsid);
+int nvme_identify_ns(struct nvme_ctrl *ctrl, unsigned nsid, struct nvme_id_ns **id);    
+void nvme_set_queue_limits(struct nvme_ctrl *ctrl, struct request_queue *q);
+void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id); 
+int nvme_submit_user_cmd(struct request_queue *q, struct nvme_command *cmd, void __user *ubuffer, unsigned bufflen, void __user *meta_buffer, unsigned meta_len, u32 meta_seed, u64 *result, unsigned timeout);
+
+#ifdef CONFIG_TREENVME
+void add_treedisk(struct nvme_ctrl *ctrl, struct nvme_ns *ns, unsigned nsid);
+void treenvme_set_name(char *disk_name, struct nvme_ns *ns, struct nvme_ctrl *ctrl, int *flags);
+int treenvme_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd, unsigned long arg);
+inline void nvme_backpath(struct nvme_queue *nvmeq, u16 idx, struct request *req, struct nvme_completion *cqe);
+#else
+void add_treedisk(struct nvme_ctrl *ctrl, struct nvme_ns *ns, unsigned nsid){
+}
+inline void nvme_backpath(struct nvme_queue *nvmeq, u16 idx, struct request *req, struct nvme_completion *cqe){
+}
+#endif /* CONFIG_NVME_MULTIPATH */
+
+
 #ifdef CONFIG_NVME_MULTIPATH
 static inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
 {
@@ -562,6 +745,9 @@ void nvme_mpath_stop(struct nvme_ctrl *ctrl);
 bool nvme_mpath_clear_current_path(struct nvme_ns *ns);
 void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl);
 struct nvme_ns *nvme_find_path(struct nvme_ns_head *head);
+int treenvme_alloc_disk(struct nvme_ctrl *ctrl,struct treenvme_head *head);
+blk_qc_t treenvme_make_request(struct request_queue *q, struct bio *bio);
+
 
 static inline void nvme_mpath_check_last_path(struct nvme_ns *ns)
 {
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index cc46e250fcac..e1ca2b62d88e 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -78,58 +78,16 @@ static unsigned int poll_queues;
 module_param(poll_queues, uint, 0644);
 MODULE_PARM_DESC(poll_queues, "Number of queues to use for polled IO.");
 
-struct nvme_dev;
-struct nvme_queue;
+static unsigned int dep_depth;
+module_param(dep_depth, uint, 0644);
+MODULE_PARM_DESC(dep_depth, "depth of dependencies."); 
+
+//struct nvme_dev;
+//struct nvme_queue;
 
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown);
 static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode);
 
-/*
- * Represents an NVM Express device.  Each nvme_dev is a PCI function.
- */
-struct nvme_dev {
-	struct nvme_queue *queues;
-	struct blk_mq_tag_set tagset;
-	struct blk_mq_tag_set admin_tagset;
-	u32 __iomem *dbs;
-	struct device *dev;
-	struct dma_pool *prp_page_pool;
-	struct dma_pool *prp_small_pool;
-	unsigned online_queues;
-	unsigned max_qid;
-	unsigned io_queues[HCTX_MAX_TYPES];
-	unsigned int num_vecs;
-	int q_depth;
-	int io_sqes;
-	u32 db_stride;
-	void __iomem *bar;
-	unsigned long bar_mapped_size;
-	struct work_struct remove_work;
-	struct mutex shutdown_lock;
-	bool subsystem;
-	u64 cmb_size;
-	bool cmb_use_sqes;
-	u32 cmbsz;
-	u32 cmbloc;
-	struct nvme_ctrl ctrl;
-	u32 last_ps;
-
-	mempool_t *iod_mempool;
-
-	/* shadow doorbell buffer support: */
-	u32 *dbbuf_dbs;
-	dma_addr_t dbbuf_dbs_dma_addr;
-	u32 *dbbuf_eis;
-	dma_addr_t dbbuf_eis_dma_addr;
-
-	/* host memory buffer support: */
-	u64 host_mem_size;
-	u32 nr_host_mem_descs;
-	dma_addr_t host_mem_descs_dma;
-	struct nvme_host_mem_buf_desc *host_mem_descs;
-	void **host_mem_desc_bufs;
-};
-
 static int io_queue_depth_set(const char *val, const struct kernel_param *kp)
 {
 	int n = 0, ret;
@@ -156,59 +114,6 @@ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
 	return container_of(ctrl, struct nvme_dev, ctrl);
 }
 
-/*
- * An NVM Express queue.  Each device has at least two (one for admin
- * commands and one for I/O commands).
- */
-struct nvme_queue {
-	struct nvme_dev *dev;
-	spinlock_t sq_lock;
-	void *sq_cmds;
-	 /* only used for poll queues: */
-	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
-	volatile struct nvme_completion *cqes;
-	dma_addr_t sq_dma_addr;
-	dma_addr_t cq_dma_addr;
-	u32 __iomem *q_db;
-	u16 q_depth;
-	u16 cq_vector;
-	u16 sq_tail;
-	u16 last_sq_tail;
-	u16 cq_head;
-	u16 qid;
-	u8 cq_phase;
-	u8 sqes;
-	unsigned long flags;
-#define NVMEQ_ENABLED		0
-#define NVMEQ_SQ_CMB		1
-#define NVMEQ_DELETE_ERROR	2
-#define NVMEQ_POLLED		3
-	u32 *dbbuf_sq_db;
-	u32 *dbbuf_cq_db;
-	u32 *dbbuf_sq_ei;
-	u32 *dbbuf_cq_ei;
-	struct completion delete_done;
-};
-
-/*
- * The nvme_iod describes the data in an I/O.
- *
- * The sg pointer contains the list of PRP/SGL chunk allocations in addition
- * to the actual struct scatterlist.
- */
-struct nvme_iod {
-	struct nvme_request req;
-	struct nvme_queue *nvmeq;
-	bool use_sgl;
-	int aborted;
-	int npages;		/* In the PRP list. 0 means small pool in use */
-	int nents;		/* Used in scatterlist */
-	dma_addr_t first_dma;
-	unsigned int dma_len;	/* length of single DMA segment mapping */
-	dma_addr_t meta_dma;
-	struct scatterlist *sg;
-};
-
 static unsigned int max_io_queues(void)
 {
 	return num_possible_cpus() + write_queues + poll_queues;
@@ -472,17 +377,19 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
  * @cmd: The command to send
  * @write_sq: whether to write to the SQ doorbell
  */
-static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
+void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
-	spin_lock(&nvmeq->sq_lock);
+	unsigned long flags;
+	spin_lock_irqsave(&nvmeq->sq_lock, flags);
 	memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
 	       cmd, sizeof(*cmd));
 	if (++nvmeq->sq_tail == nvmeq->q_depth)
 		nvmeq->sq_tail = 0;
 	nvme_write_sq_db(nvmeq, write_sq);
-	spin_unlock(&nvmeq->sq_lock);
+	spin_unlock_irqrestore(&nvmeq->sq_lock, flags);
 }
+EXPORT_SYMBOL_GPL(nvme_submit_cmd);
 
 static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
 {
@@ -791,7 +698,7 @@ static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 	return 0;
 }
 
-static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
+blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -841,8 +748,9 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 		nvme_unmap_data(dev, req);
 	return ret;
 }
+EXPORT_SYMBOL_GPL(nvme_map_data);
 
-static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
+blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -854,6 +762,7 @@ static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 	cmnd->rw.metadata = cpu_to_le64(iod->meta_dma);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(nvme_map_metadata); 
 
 /*
  * NOTE: ns is NULL when called on the admin queue.
@@ -968,7 +877,65 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 
 	req = blk_mq_tag_to_rq(nvme_queue_tagset(nvmeq), cqe->command_id);
 	trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
-	nvme_end_request(req, cqe->status, cqe->result);
+
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	//struct nvme_queue *nvmeq = iod->nvmeq;
+	struct nvme_ns *ns = req->q->queuedata;
+	struct nvme_dev *dev = iod->nvmeq->dev;
+	struct nvme_command cmnd;
+	blk_status_t ret;
+
+	if (req->cmd_flags & REQ_TREENVME)
+	{
+		/*
+		printk(KERN_ERR "GOT HERE -- rebound \n");
+		if (req->alter_count < dep_depth)
+		{
+			req->alter_count += 1;
+			// alter
+			ret = nvme_setup_cmd(ns, req, &cmnd);
+			if (ret)
+				printk(KERN_ERR "submit error\n");
+			//printk(KERN_ERR "Got here 2\n");x
+			if (blk_rq_nr_phys_segments(req)) {
+				ret = nvme_map_data(dev, req, &cmnd);
+				if (ret)
+					printk(KERN_ERR "mapping error\n");
+			}
+			if (blk_integrity_rq(req)) {
+				ret = nvme_map_metadata(dev, req, &cmnd);
+				if (ret)
+					printk(KERN_ERR "meta error\n");
+			}
+			cmnd.rw.slba = cpu_to_le64(nvme_sect_to_lba(ns, blk_rq_pos(req)));
+			nvme_req(req)->cmd = &cmnd;
+			printk(KERN_ERR "SECTOR NUMBER IS %d\n", cmnd.rw.slba);
+
+			int ret;
+			struct bio_vec bvec;
+			struct req_iterator iter;
+
+			rq_for_each_segment(bvec, req, iter)
+			{
+				char *buffer = bio_data(req->bio);
+				printk(KERN_ERR "char bio: %s \n", buffer);
+			}
+			nvme_submit_cmd(nvmeq, &cmnd, true);
+		}
+		else
+		{
+			//printk(KERN_ERR "Final\n");
+			req = blk_mq_tag_to_rq(nvme_queue_tagset(nvmeq), req->first_command_id);
+			nvme_end_request(req, cqe->status, cqe->result);
+		}
+		*/
+		nvme_backpath(nvmeq, idx, req, cqe);
+	}
+	else {
+		req = blk_mq_tag_to_rq(nvme_queue_tagset(nvmeq), req->first_command_id);
+		nvme_end_request(req, cqe->status, cqe->result);
+	}
+	
 }
 
 static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
@@ -2100,7 +2067,7 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 
 	if (nr_io_queues == 0)
 		return 0;
-	
+
 	clear_bit(NVMEQ_ENABLED, &adminq->flags);
 
 	if (dev->cmb_use_sqes) {
diff --git a/drivers/nvme/host/tokuspec.h b/drivers/nvme/host/tokuspec.h
new file mode 100644
index 000000000000..89c881778c74
--- /dev/null
+++ b/drivers/nvme/host/tokuspec.h
@@ -0,0 +1,115 @@
+// This is a key-data structure..
+
+#include <linux/hashtable.h>
+
+struct DBT {
+	void *data;		/* key value */
+	uint32_t size;		/* key/data length */
+	uint32_t ulen;		/* read-only: length of user buffer */
+	uint32_t dlen;		/* read-only: get/put record length */
+	uint32_t doff;		/* read-only: get/put record offset */
+	void *app_data;
+	uint32_t flags;
+	uint32_t blocknum;
+};
+
+struct pivot_bounds {
+	int num_pivots;
+	int total_size;
+	char *fixed_keys;
+	int fixed_keylen;
+	int fixed_keylen_aligned;
+	struct DBT *dbt_keys;
+};
+
+struct block_data {
+	uint32_t start;
+	uint32_t end;
+	uint32_t size;
+};
+
+enum child_tag {
+	SUBBLOCK = 1,
+	BASEMENT = 2,
+};
+
+struct subblock_data {
+	void *ptr;
+	uint32_t csize; // compressed size
+	uint32_t usize; // uncompressed size
+};
+
+struct basement_data {
+	uint32_t le_offset;
+	uint8_t key[0];
+	struct hlist_node node;
+};
+
+struct child_node_data {
+	int blocknum;
+	union {
+		struct subblock_data *sublock;
+		struct basement_data *leaf;
+	} u;
+	enum child_tag tag;
+};
+
+struct ctpair;
+
+struct tokunode {
+	int max_msn_applied_to_node_on_disk;
+	unsigned int flags;
+	uint64_t blocknum;
+	int layout_version;
+	int layout_version_original;
+	int layout_version_read_from_disk;
+	uint32_t build_id;
+	int height;
+	int dirty_;
+	uint32_t fullhash;
+	int n_children;
+	struct pivot_bounds *pivotkeys;
+	struct child_node_data *cnd;
+	struct ctpair *ct_pair;
+};
+
+typedef int (*comparator)(char *a, char *b, int asize, int bsize);
+
+enum search_direction {
+	LEFT_TO_RIGHT,
+	RIGHT_TO_LEFT
+};
+
+struct search_ctx {
+	comparator compare;
+	enum search_direction direction;
+       	const struct DBT *k;
+	void *user_data;
+	struct DBT *pivot_bound;	
+	const struct DBT *k_bound;
+};
+
+enum translation_type {
+	TRANSLATION_NONE = 0,
+	TRANSLATION_CURRENT,
+	TRANSLATION_INPROGRESS,
+	TRANSLATION_CHECKPOINTED,
+	TRANSLATION_DEBUG
+};
+
+struct block_translation_pair {
+	union {
+		uint64_t diskoff;
+		uint64_t free_blocknum;	
+	} u;
+
+	uint64_t size;
+};
+
+struct block_table {
+	enum translation_type type;
+	int64_t length_of_array;
+	int64_t smallest;
+	int64_t next_head;
+	struct block_translation_pair *block_translation; 
+};
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 32868fbedc9e..a9b4b21d9a26 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -244,6 +244,10 @@ struct request {
 	 */
 	rq_end_io_fn *end_io;
 	void *end_io_data;
+
+	unsigned int alter_count;
+	unsigned int total_count;
+	__u16 first_command_id;
 };
 
 static inline bool blk_op_is_scsi(unsigned int op)
diff --git a/include/uapi/linux/treenvme.h b/include/uapi/linux/treenvme.h
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/include/uapi/linux/treenvme_ioctl.h b/include/uapi/linux/treenvme_ioctl.h
new file mode 120000
index 000000000000..3cf44aebed6a
--- /dev/null
+++ b/include/uapi/linux/treenvme_ioctl.h
@@ -0,0 +1 @@
+../../../drivers/nvme/host/api/treenvme_ioctl.h
\ No newline at end of file
-- 
2.25.1

